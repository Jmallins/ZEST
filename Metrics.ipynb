{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metrics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Z3ehzYmlZMZt",
        "outputId": "ae97f24e-ae1f-44b6-a0e8-cb5d2e352edc"
      },
      "source": [
        "\"\"\"Copyright 2020-2021 Google LLC\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Copyright 2020-2021 Google LLC\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n    https://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YjY68H0wF7W"
      },
      "source": [
        "!pip install pyphen\n",
        "import pyphen\n",
        "!pip install mosestokenizer\n",
        "from mosestokenizer import *\n",
        "!pip install scipy\n",
        "german_sentence_splitter = MosesSentenceSplitter('de')  \n",
        "german_tokenizer  = MosesTokenizer('de')\n",
        "german_dictionary = pyphen.Pyphen(lang='de_DE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceX9rrija0ei",
        "outputId": "f6c0d925-0b19-4539-d941-6dc4b938c411"
      },
      "source": [
        "# FKRE\n",
        "from typing import Sequence\n",
        "import string\n",
        "def fkre(sentences: Sequence[str]):\n",
        "  fkre = 0.0\n",
        "  for sentence in sentences:\n",
        "    sentence = setence.strip()\n",
        "    if not sentence:\n",
        "      sentence = \".\"\n",
        "    number_of_sentences = max(1,len(german_sentence_splitter([sentence]))) # Can't be less than one sentence. \n",
        "    tokens = german_tokenizer(sentence)\n",
        "    number_of_words = 0\n",
        "    number_of_syllables = 0\n",
        "    for token in tokens:\n",
        "      if token in string.punctuation: # We don't count punc towards syllables  or word count. \n",
        "        continue \n",
        "      number_of_words+=1\n",
        "      number_of_syllables+=len(german_dictionary.inserted(token).split(\"-\"))\n",
        "    number_of_words = max(1,number_of_words) # We assume there is at least one word. \n",
        "    fkre+=180 - (number_of_words/number_of_sentences) - (58.5 *(number_of_syllables/number_of_words))\n",
        "  return fkre/len(sentences)\n",
        "\n",
        "fkre([\"Ingrid Persdotter ist der Name einer schwedi­schen Nonne, die 1498 im Kloster Vadstena (Bild) einen stil- und 1.\",\"Ingrid Persdotter ist der Name einer schwedi­schen Nonne, die 1498 im Kloster Vadstena (Bild) einen stil- und 1.\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.39999999999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04zSa-tCfLBE"
      },
      "source": [
        "import subprocess\n",
        "multi_bleu = \"multi-bleu-detok.perl\" # Path to multibleu. You can get it from https://github.com/EdinburghNLP/nematus   \n",
        "\n",
        "def calculate_bleu(output_file,reference_file):\n",
        "  command = multi_bleu + \" \" + reference_file +\" < \" + output_file +\" |  cut -f 3 -d ' ' | cut -f 1 -d ','\"\n",
        "  ps = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE)\n",
        "  output = ps.communicate()[0]\n",
        "  return float(output.strip())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBIN6_-kzTu"
      },
      "source": [
        "# All files should be de-tokenized.\n",
        "from scipy.special import expit\n",
        "\n",
        "def score(source_file,output_file,reference_file):\n",
        "  bleu = calculate_bleu(output_file,reference_file)\n",
        "  ibleu = (bleu *0.9) - (calculate_bleu(output_file,source_file) * 0.1)\n",
        "  source = open(source_file)\n",
        "  fkre_source = fkre(source.readlines().strip().split())\n",
        "  fkre_output = fkre(source.readlines().strip().split())\n",
        "  fk_bleu = expit(((fkre_source- fkre_output) **0.5 )) * ( (ibleu/100.0)**0.5)\n",
        "  return {\"bleu\":bleu,\"ibleu\":ibleu,\"fk-bleu\":fk_bleu}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW_tXCqor4OR",
        "outputId": "309e8a9a-b561-4f96-f882-36c1f616c736"
      },
      "source": [
        "# For SARI we tokenized using MosesTokenizer(\"de\") the source, output, and ref \n",
        "# https://github.com/apache/joshua can then be used to calculate corpus level SARI.\n",
        "# Alternatively you can use the following sentence based sari. \n",
        "\n",
        "# =======================================================\n",
        "#  SARI -- Text Simplification Tunable Evaluation Metric\n",
        "# =======================================================\n",
        "#\n",
        "# Author: Wei Xu (UPenn xwe@cis.upenn.edu)\n",
        "#\n",
        "# A Python implementation of the SARI metric for text simplification\n",
        "# evaluation in the following paper  \n",
        "#\n",
        "#     \"Optimizing Statistical Machine Translation for Text Simplification\"\n",
        "#     Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen and Chris Callison-Burch\n",
        "#     In Transactions of the Association for Computational Linguistics (TACL) 2015\n",
        "# \n",
        "# There is also a Java implementation of the SARI metric \n",
        "# that is integrated into the Joshua MT Decoder. It can \n",
        "# be used for tuning Joshua models for a real end-to-end\n",
        "# text simplification model. \n",
        "#\n",
        "\n",
        "from __future__ import division\n",
        "from collections import Counter\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "def ReadInFile (filename):\n",
        "    \n",
        "    with open(filename) as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [x.strip() for x in lines]\n",
        "    return lines\n",
        "\n",
        "\n",
        "def SARIngram(sgrams, cgrams, rgramslist, numref):\n",
        "    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n",
        "    rgramcounter = Counter(rgramsall)\n",
        "\t\n",
        "    sgramcounter = Counter(sgrams)\n",
        "    sgramcounter_rep = Counter()\n",
        "    for sgram, scount in sgramcounter.items():\n",
        "        sgramcounter_rep[sgram] = scount * numref\n",
        "        \n",
        "    cgramcounter = Counter(cgrams)\n",
        "    cgramcounter_rep = Counter()\n",
        "    for cgram, ccount in cgramcounter.items():\n",
        "        cgramcounter_rep[cgram] = ccount * numref\n",
        "\t\n",
        "    \n",
        "    # KEEP\n",
        "    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n",
        "    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n",
        "    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n",
        "\n",
        "    keeptmpscore1 = 0\n",
        "    keeptmpscore2 = 0\n",
        "    for keepgram in keepgramcountergood_rep:\n",
        "        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n",
        "        keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n",
        "        #print \"KEEP\", keepgram, keepscore, cgramcounter[keepgram], sgramcounter[keepgram], rgramcounter[keepgram]\n",
        "    keepscore_precision = 0\n",
        "    if len(keepgramcounter_rep) > 0:\n",
        "    \tkeepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n",
        "    keepscore_recall = 0\n",
        "    if len(keepgramcounterall_rep) > 0:\n",
        "    \tkeepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n",
        "    keepscore = 0\n",
        "    if keepscore_precision > 0 or keepscore_recall > 0:\n",
        "        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n",
        "\n",
        "\n",
        "    # DELETION\n",
        "    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n",
        "    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n",
        "    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n",
        "    deltmpscore1 = 0\n",
        "    deltmpscore2 = 0\n",
        "    for delgram in delgramcountergood_rep:\n",
        "        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n",
        "        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n",
        "    delscore_precision = 0\n",
        "    if len(delgramcounter_rep) > 0:\n",
        "    \tdelscore_precision = deltmpscore1 / len(delgramcounter_rep)\n",
        "    delscore_recall = 0\n",
        "    if len(delgramcounterall_rep) > 0:\n",
        "    \tdelscore_recall = deltmpscore1 / len(delgramcounterall_rep)\n",
        "    delscore = 0\n",
        "    if delscore_precision > 0 or delscore_recall > 0:\n",
        "        delscore = 2 * delscore_precision * delscore_recall / (delscore_precision + delscore_recall)\n",
        "\n",
        "\n",
        "    # ADDITION\n",
        "    addgramcounter = set(cgramcounter) - set(sgramcounter)\n",
        "    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n",
        "    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n",
        "\n",
        "    addtmpscore = 0\n",
        "    for addgram in addgramcountergood:\n",
        "        addtmpscore += 1\n",
        "\n",
        "    addscore_precision = 0\n",
        "    addscore_recall = 0\n",
        "    if len(addgramcounter) > 0:\n",
        "    \taddscore_precision = addtmpscore / len(addgramcounter)\n",
        "    if len(addgramcounterall) > 0:\n",
        "    \taddscore_recall = addtmpscore / len(addgramcounterall)\n",
        "    addscore = 0\n",
        "    if addscore_precision > 0 or addscore_recall > 0:\n",
        "        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n",
        "    \n",
        "    return (keepscore, delscore_precision, addscore)\n",
        "    \n",
        "\n",
        "def SARIsent (ssent, csent, rsents) :\n",
        "    ssent = \" \".join(german_tokenizer(ssent))\n",
        "    csent = \" \".join(german_tokenizer(csent))\n",
        "    rsents = [\" \".join(german_tokenizer(rsent)) for rsent in rsents]\n",
        "    numref = len(rsents)\t\n",
        "\n",
        "    s1grams = ssent.lower().split(\" \")\n",
        "    c1grams = csent.lower().split(\" \")\n",
        "    s2grams = []\n",
        "    c2grams = []\n",
        "    s3grams = []\n",
        "    c3grams = []\n",
        "    s4grams = []\n",
        "    c4grams = []\n",
        " \n",
        "    r1gramslist = []\n",
        "    r2gramslist = []\n",
        "    r3gramslist = []\n",
        "    r4gramslist = []\n",
        "    for rsent in rsents:\n",
        "        r1grams = rsent.lower().split(\" \")    \n",
        "        r2grams = []\n",
        "        r3grams = []\n",
        "        r4grams = []\n",
        "        r1gramslist.append(r1grams)\n",
        "        for i in range(0, len(r1grams)-1) :\n",
        "            if i < len(r1grams) - 1:\n",
        "                r2gram = r1grams[i] + \" \" + r1grams[i+1]\n",
        "                r2grams.append(r2gram)\n",
        "            if i < len(r1grams)-2:\n",
        "                r3gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2]\n",
        "                r3grams.append(r3gram)\n",
        "            if i < len(r1grams)-3:\n",
        "                r4gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2] + \" \" + r1grams[i+3]\n",
        "                r4grams.append(r4gram)        \n",
        "        r2gramslist.append(r2grams)\n",
        "        r3gramslist.append(r3grams)\n",
        "        r4gramslist.append(r4grams)\n",
        "       \n",
        "    for i in range(0, len(s1grams)-1) :\n",
        "        if i < len(s1grams) - 1:\n",
        "            s2gram = s1grams[i] + \" \" + s1grams[i+1]\n",
        "            s2grams.append(s2gram)\n",
        "        if i < len(s1grams)-2:\n",
        "            s3gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2]\n",
        "            s3grams.append(s3gram)\n",
        "        if i < len(s1grams)-3:\n",
        "            s4gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2] + \" \" + s1grams[i+3]\n",
        "            s4grams.append(s4gram)\n",
        "            \n",
        "    for i in range(0, len(c1grams)-1) :\n",
        "        if i < len(c1grams) - 1:\n",
        "            c2gram = c1grams[i] + \" \" + c1grams[i+1]\n",
        "            c2grams.append(c2gram)\n",
        "        if i < len(c1grams)-2:\n",
        "            c3gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2]\n",
        "            c3grams.append(c3gram)\n",
        "        if i < len(c1grams)-3:\n",
        "            c4gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2] + \" \" + c1grams[i+3]\n",
        "            c4grams.append(c4gram)\n",
        "\n",
        "\n",
        "    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref)\n",
        "    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref)\n",
        "    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref)\n",
        "    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref)\n",
        "    avgkeepscore = sum([keep1score,keep2score,keep3score,keep4score])/4\n",
        "    avgdelscore = sum([del1score,del2score,del3score,del4score])/4\n",
        "    avgaddscore = sum([add1score,add2score,add3score,add4score])/4\n",
        "    finalscore = (avgkeepscore + avgdelscore + avgaddscore ) / 3\n",
        "\n",
        "    return finalscore\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    ssent = \"About 95 species are currently accepted .\"\n",
        "    csent1 = \"About 95 you now get in .\"\n",
        "    csent2 = \"About 95 species are now agreed .\"\n",
        "    csent3 = \"About 95 species are currently agreed .\"\n",
        "    rsents = [\"About 95 species are currently known .\", \"About 95 species are now accepted .\", \"95 species are now accepted .\"]\n",
        "\n",
        "    print(SARIsent(ssent, csent1, rsents))\n",
        "    print(SARIsent(ssent, csent2, rsents))\n",
        "    print(SARIsent(ssent, csent3, rsents))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2682782411698074\n",
            "0.5889995423074248\n",
            "0.5071608864657479\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}